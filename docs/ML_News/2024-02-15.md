---
layout: default
title: 2024-02-15 Sora, Gemini 1.5, V-JEPA
parent: ML News
---
# 2024-02-15 Sora, Gemini 1.5, V-JEPA

## Sora: Video Generation Models as World Simulators
- Reference: <a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a>
- Reference: <a href="https://openai.com/sora">https://openai.com/sora</a>

### Summary
- DiT (TODO)


## Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context
- Reference: <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15">https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15</a>
- Reference: <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf</a>


### Summary
- 10 million context sizes
- MOE




## V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video
- Reference: <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video">https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video</a>
- Reference: <a href="https://github.com/facebookresearch/jepa">https://github.com/facebookresearch/jepa</a>

### Summary
#### Algorithm
V-JEPA uses a non-generative loss for video representation learning. During training, a contiguous video clips $$V=(v_1,\cdots,v_T)$$ of $$T$$ frames are spatially partitioned into disjoint set of context $$X=(x_1,\cdots,x_T)$$ and target $$Y=(y_1,\cdots,y_T)$$. Both $$X$$ and $$Y$$ are cut into patches and flattened into a sequence of $$L$$ patches (or tokens). 

Then we obtain context features $$E_\theta(X)$$ and target features $$\text{sg}(\overline{E}_\theta(Y))$$ using the ViT encoders $$E_\theta(\cdot)$$, where $$\text{sg}(\cdot)$$ is the stop gradient operator and $$\overline{E}_\theta(\cdot)$$ represents an exponential moving average. This is used by previous works in vision SSL to prevent representational collapses with asymmetric architectures. 

The hope is that context features $$E_\theta(X)$$ are predictive of target features $$\text{sg}(\overline{E}_\theta(Y))$$. A projector network $$P_\phi(E_\theta(X), \Delta_Y)$$ parametrized by a small Transformer takes inputs $$E_\theta(X)$$ and positional embeddings of the masked spatio-temporal patches $$\Delta_Y$$. The goal is that after the forward pass of $$P_\phi(\cdot)$$, we output a vector with the same shape and content as $$\text{sg}(\overline{E}_\theta(Y))$$. Thus the loss function is given by

$$\min_{\theta,\phi} \|P_\phi(E_\theta(X), \Delta_Y) - \text{sg}(\overline{E}_\theta(Y))\|_1$$

This is really similar to MAE but doing mask prediction in the latent space.

#### Empirical Results
- Table 1: pixel-space models are not too far away from latent space models in frozen evaluation. For finetuning, the performances basically match. It's unclear what will be the performance at scale, but it makes sense to do prediction in a latent space.
- Table 4: It does look like the masking strategies here for the creation of $X$ and $Y$ has to be invariant across the temporal dimensions to prevent information leakage.
- Table 5: V-JEPA does great for frozen evaluations. Under the finetuning regime, the advantages are not clear. 

