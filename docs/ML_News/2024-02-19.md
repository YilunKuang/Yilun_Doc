---
layout: default
title: 2024-02-19 BioMistral
parent: ML News
---
# 2024-02-19 BioMistral

## BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains

<!-- - Reference: https://huggingface.co/BioMistral/BioMistral-7B
- Reference: https://arxiv.org/pdf/2402.10373.pdf -->
- Reference: <a href="https://huggingface.co/BioMistral/BioMistral-7B">https://huggingface.co/BioMistral/BioMistral-7B</a>
- Reference: <a href="https://arxiv.org/pdf/2402.10373.pdf">https://arxiv.org/pdf/2402.10373.pdf</a>

### Summary

- BioMistral is based on Mistral as a foundation model and further pre-trained on PubMed Central.
- BioMistral is evaluated on a benchmark comprising 10 established medical question-answering (QA) tasks with other LLMs pretrained with biological data. After model merging, BioMistral seems to be doing the best among 7B LLMs, although GPT 3.5 still does better. They also didn't compare to 13B and 70B LLaMA2 models. It's still hard to tell between these models since pretraining is not controlled.
