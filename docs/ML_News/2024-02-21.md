---
layout: default
title: 2024-02-21 Gemma
parent: ML News
---
# 2024-02-21 Gemma

## Gemma: Open Models Based on Gemini
Research and Technology

- Reference: <a href="https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf">https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf</a>
- Reference: <a href="https://huggingface.co/blog/gemma">https://huggingface.co/blog/gemma</a>

### Summary
- Gemma 2B/7B-(Instruction Tuned + RLHF)
- training set size: 6T tokens of text
- Models are trained on a context length of 8192 tokens.
- vocabulary size is 256k tokens.
- "Following Gemini, we find that including subsets of data that encourage better in-context attribution, hedging, and refusals to minimize hallucinations can improve performance on several factuality metrics, without degrading model performance on other metrics."
- Gemma 7B mostly outperforms Mistral 7B on selected evaluation sets


