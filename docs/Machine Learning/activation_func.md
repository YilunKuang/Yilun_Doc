---
layout: default
title: Activation Functions
parent: Machine Learning
---

# Activation Functions

## SwiGLU

TODO

**Reference**
- [1] GLU Variants Improve Transformer. https://arxiv.org/abs/2002.05202
- [2] PaLM: Scaling Language Modeling with Pathways. https://arxiv.org/abs/2204.02311
  - PaLM used SwiGLU for MLPs in Transformers
- [3] Llama 2: Open Foundation and Fine-Tuned Chat Models. https://arxiv.org/abs/2307.09288
  - Llama 2 also used SwiGLU for MLPs in Transformers
- [4] Theory, Analysis, and Best Practices for Sigmoid Self-Attention. https://arxiv.org/abs/2409.04431
  - Sigmoid activation for attention
